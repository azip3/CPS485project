# CPS485project

## Description
(Basic Proposal/subject to change)
My project idea is a live ASL gesture recognition application. It would use a Convolutional Neural Network  to classify American Sign Language hand gestures from webcam video input. The system would detect hand landmarks, processes frames, and displays the predicted sign in real time.

Key features:
- Real-time gesture detection via webcam
- Trained neural network for high-accuracy classification
- User-friendly interface for live demonstration
- Possible visualization of training progress (loss/accuracy curves)

possible timeline until midterm
(weeks starting from February 3)
- Week 1-2/Data Acquisition and Model Development: Collect ASL datasets from Kaggle and start buliding CNN architecture;Train on static images; Initial hyperparameter tuning.
- Week 3-4/Integration & Real-time Testing: Link the model to OpenCV; Implement MediaPipe for landmark detection.
- Week 4-5/UI Design & Optimization: Build the user interface; Optimize for latency; Finalize loss/accuracy visualizations.
- Week 6( just week 6 if midterm is week 7): Final bug fixes